\section{Derivation of Autoencoder}
We skip the description of feedforward neural network with supervised learning but begin with deducing autoencoder directly.~Suppose that we have only unlabeled training examples set $\{x^{(1)},x^{(2)},x^{(3)},\cdots\}$, where $x^{(i)} \in \mathbb{R}$.~An autoencoder neural network (see Fig.~\ref{fig:autoencoder}) is an unsupervised learning algorithm that applies backpropagation, setting the outputs to be equivalent to the inputs, i.e., $y^{(i)}=x^{(i)}$.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.3\textwidth]{figures/autoencoder.png}
	\caption{An autoencoder architecture} \label{fig:autoencoder}
\end{figure}

The autoencoder tries to learn a function $h_{W,b} (\bm{x}) \approx x$.~In other words, it desires to learn an approximation to the identity function, so as to output $\hat{\bm{x}}$ that is similar to $\bm{x}$.~The identity function seems a particularly trivial function to be done pointlessly; but by placing constraints on the network, such as by limiting the number of hidden units, we can discover potential structure about the data.~As a concrete instance, assume the inputs $\bm{x}$ are the pixel intensity values from a $10 \times 10$ image (100 pixels) so $n=100$, and there are $s_2=50$ hidden units in layer $L_2$.~Note that we also have $\bm{y} \in \mathbb{R}^{100}$ as the output.~Since there are only 50 hidden units, the network is forced to perceive a \textbf{compressed} representation of the input, i.e., given only the vector of activations of hidden units $\bm{a}^{(2)} \in \mathbb{R}^{50}$, it must endeavor to \textbf{reconstruct} the 100-pixel input $\bm{x}$.~If the input were completely stochastic---say, each $x_i$ comes from an i.i.d. Gaussian distribution independent of the other features---then this compression task would be quite difficult.~But if there is underlying structure in the data, e.g., if some of the input features are correlated, then this algorithm will be able to discover some of those correlations.

\subsection{Sparsity}
One argument above relies on the number of hidden units $s_2$ being small.~But even when the number of hidden units is large (perhaps even greater than the number of input pixels), we can still discover latent structure, by imposing other constraints on the network.~In particular, if we impose a \textbf{sparsity} constraint on the hidden units, then the autoencoder will still detect potential structure in the data, even if the number of hidden units is large.

Informally, we will think of a neuron as being ``active'' (or ``firing'') if its output value is close to 1, or as being ``inactive'' if its output value is close to 0\footnote{This statement assumes a sigmoid activation function.~As for a tanh activation function, we think of a neuron as being inactive when its output values near to -1.}.~We would like to constrain the neurons to be inactive most of the time.

Recall that $a_j^{(2)}$ denotes the activation of hidden unit $j$ in the autoencoder.~However, this notation does not make explicit what is the input $\bm{x}$ that leads to this activation.~Thus, we rewrite $a_j^{(2)}(\bm{x})$ to signify the activation of this hidden unit when the network is given a specific input $\bm{x}$.~Further, let
\begin{align}
	\hat{\rho}_j = \frac{1}{m} \sum_{i=1}^{m}[a_j^{(2)}(x^{(i)})]
\end{align}
be the average activation of hidden unit $j$ (averaged over the training set).~We would like to approximately enforce the constraint
\begin{align}
	\hat{\rho}_j = \rho,
\end{align}
where $\rho$ is a \textbf{sparsity parameter}, typically a small value close to zero (e.g. $\rho = 0.05$).~In other words, we would like the average activation of each hidden neuron $j$ to be close to 0.05.~To satisfy this constraint, the hidden unit's activations must mostly be near 0.

To achieve this, we will add an extra penalty term to our optimization objective given in \eqref{eq:objective} that penalizes $\hat{\rho}_j$ deviating from $\rho$ considerably.~Many choices of the penalty term will give reasonable results.~We will choose the following:
\begin{align}
	\sum_{j=1}^{s_2} \rho \log \frac{\rho}{\hat{\rho}_j} + (1 - \rho) \log \frac{1 - \rho}{1 - \hat{\rho}_j}.
\end{align}
Here, $s_2$ is the number of neurons in the hidden layer, and the index $j$ is summing over the hidden units in our network.~If you are familiar with the concept of KL divergence, this penalty term is based on it, and can also be written
\begin{align}
	\sum_{j=1}^{s_2} \text{KL} (\rho \parallel \hat{\rho}_j),
\end{align}
where $\text{KL} (\rho \parallel \hat{\rho}_j)$ is the Kullback-Leibler (KL) divergence between a Bernoulli random variable with mean $\rho$ and a Bernoulli random variable with mean $\hat{\rho}_j$.~KL divergence is a standard function for measuring how distinct two different distributions are.

This penalty function has the property that $\text{KL} (\rho \parallel \hat{\rho}_j) = 0$ if $\hat{\rho}_j = \rho$, and otherwise it increases monotonically as $\hat{\rho}_j$ deviates from $\rho$.~For instance, in Fig.~\ref{fig:KL}, we have set $\rho = 0.2$, and plotted $\text{KL} (\rho \parallel \hat{\rho}_j)$ for $\hat{\rho}_j$ ranging from $[0,1]$.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.35\textwidth]{figures/KL_divergence.png}
	\caption{Plot KL divergence with $\rho = 0.2$} \label{fig:KL}
\end{figure}

We find that the KL divergence reaches its minimum of 0 at $\hat{\rho}_j = \rho$, and rises promptly (actually approaches $\infty$) as $\hat{\rho}_j$ closes to 0 or 1.~Thus, minimizing this penalty term has the effect of causing $\hat{\rho}_j$ to approximate to $\rho$.

\subsection{Optimization}
Then we formulate our problem to the standard form yields
\begin{align}
	\text{minimize} \quad & \frac{1}{m} \sum_{t=1}^m \frac{1}{2} \parallel h_{W,b}(x^{(t)})- x^{(t)} \parallel ^2 \label{eq:objective} \\
	\text{subject to} \quad & \quad \parallel \bm{W} \parallel ^2 \ \leqslant \epsilon, \label{eq:weight_decay} \\
	& \quad \hat{\rho}_j = \rho,~j=1,2,\cdots,s_2. 
\end{align}
Clearly we will minimize the objective as mean-square errors.~It is accompanied by two constraints:~(1) The magnitude of weight $\bm{W}$ should be small;~(2) Average activation of each hidden unit $\hat{\rho}_j$ ought to be sparse.~Note that $h_{W,b}(\bm{x})$, which is regraded as a re-constructive function of the input $\bm{x}$, will be influenced by $\bm{W}^{(1)},\bm{W}^{(2)}$ significantly.~But we do not comprehend how weight $\bm{W}$ works on the objective.~In short, we can \textbf{not} prove whether the problem proposed in \eqref{eq:objective} is \textbf{convex} or not.~Thus, the close form of weight $\bm{W}$ will hardly be expressed, directly.

Despite the convexity of our problem being unknown, that is, we can not figure out the global optimal solution for $\bm{W}$ straightforwardly, it is still feasible to employ the \textbf{gradient descent method}, one of the effective but not the best techniques, to solve the problem.

With a variety of elements in $\bm{W}$, we do not ensure that $\bm{W}$ converges eventually to its global optimal solution instead of local optimal solution.~Practically, it is pretty probable to obtain the latter situation but usually performs fairly well.
\subsection{Backpropagation}
Our overall cost function is defined as
\begin{align}
J(\bm{W},\bm{b}) = \frac{1}{m} \sum_{t=1}^m \frac{1}{2} \parallel h_{W,b}(x^{(t)})- x^{(t)} \parallel ^2 + \frac{\lambda}{2} \sum_{\ell=1}^{n_\ell-1} \sum_{i=1}^{s_\ell} \sum_{j=1}^{s_{\ell+1}} \left( W^{\ell}_{ji} \right)^2 + \beta \sum_{j=1}^{s_2} {\rm KL}(\rho \parallel \hat\rho_j).
\end{align}
The first term in the definition $J(\bm{W},\bm{b})$ is an average sum-of-squares error term over $m$ training samples, followed by a regularization term (also called a \textbf{weight decay} term) that tends to decrease the magnitude of the weights, and assists prevent over-fitting.~The parameters $\lambda$ and $\beta$ control the relative significance of three terms.~The term $\hat{\rho}_j$ implicitly depends on $\bm{W},\bm{b}$, for it is the average activation of hidden unit $j$, which is acquired through $\bm{W},\bm{b}$.

One iteration of gradient descent updates the parameters $\bm{W},\bm{b}$ given by,
\begin{align}
	W_{ji}^{\ell} &:= W_{ji}^{\ell} - \alpha \frac{\partial J}{\partial W_{ji}^{\ell}}, \\
	b_j^\ell &:= b_j^\ell - \alpha \frac{\partial J}{\partial b_j^\ell}.
\end{align}
Here $\alpha$ denotes the learning rate.~From that, The key step is computing the partial derivatives above.~In consideration of multi-layer architecture (will be introduced in Section~\ref{sec:stacked}), we adopt \textbf{error backpropagation algorithm} uniformly to simplify our inference.

The ``error'' which propagates backwards through the network can be thought of as ``sensitivities'' with respect to the current total input of each unit, yields
\begin{align}
	\bm{\delta}^\ell = \frac{\partial J}{\partial \bm{z}^\ell},~\bm{a}^\ell = f(\bm{z}^\ell).
\end{align}
Let us consider the autoencoder network as Fig.~\ref{fig:train_autoencoder} demonstrates.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.28\textwidth]{figures/backprop.png}
	\caption{Train an autoencoder} \label{fig:train_autoencoder}
\end{figure}
We derive the sensitivity with respect to each unit from last layer to the hidden layer,
\begin{align}
\delta^{(3)}_i &= \frac{\partial J}{\partial z_i^{(3)}}= \frac{1}{m} \sum_{t=1}^{m}( h_{W,b}(x^{(t)})- x^{(t)} ) f'(z^{(3)}_i), \\
\delta^{(2)}_j = \frac{\partial J}{\partial z_j^{(2)}} &= \left( \left( \sum_{i=1}^{s_{2}} W^{(2)}_{ij} \delta^{(3)}_i \right) + \beta \left( - \frac{\rho}{\hat\rho_j} + \frac{1-\rho}{1-\hat\rho_j} \right) \right) f'(z^{(2)}_j) .
\end{align}
Note that we will need to know $\hat{\rho}_j$ to compute this term.~Thus, we have to go through a forward pass on all training examples first to calculate the average activations on the entire training set, before computing backpropagation on any sample.

Having gained the sensitivities in each layer of an autoencoder, we write the gradients for $\bm{W}^{\ell}$ and $\bm{b}^{\ell}$ of each layer, respectively, as a concise form given by
\begin{align}
\frac{\partial J}{\partial W_{ij}^{(2)}} &= \frac{\partial J}{\partial z_i^{(3)}}  \frac{\partial z_i^{(3)}}{\partial W_{ij}^{(2)}} = \delta^{(3)}_i a_{j}^{(2)} + \lambda W_{ij}^{(2)},~\frac{\partial J}{\partial b_{i}^{(2)}} = \frac{\partial J}{\partial z_i^{(3)}} \frac{\partial z_i^{(3)}}{\partial b_{i}^{(2)}} = \delta^{(3)}_i. \\
\frac{\partial J}{\partial W_{ji}^{(1)}} &= \frac{\partial J}{\partial z_j^{(2)}} \frac{\partial z_j^{(2)}}{\partial W_{ji}^{(1)}} = \delta^{(2)}_j x_{i} + \lambda W_{ji}^{(1)},~\frac{\partial J}{\partial b_{j}^{(1)}} = \frac{\partial J}{\partial z_j^{(2)}} \frac{\partial z_j^{(2)}}{\partial b_{j}^{(1)}} = \delta^{(2)}_j.
\end{align}
So far we may train the parameters of an autoencoder network in an effective way through gradient descent method.

