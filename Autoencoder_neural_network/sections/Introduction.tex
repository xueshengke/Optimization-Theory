\section{Introduction}
Supervised learning is one of the most powerful tools of Artificial Intelligence, and has led to automatic zip code recognition, speech identification, self-driving cars, and continually improving comprehension of the human genome.~Despite its significant successes, supervised learning present is still severely limited.~More specifically, most applications of it require that we manually specify the input features $x$ given to the algorithm.~Once a good feature representation is obtained, a supervised learning algorithm can work well.~But in such domains as computer vision, audio processing, and natural language processing, there are hundreds or perhaps thousands of researches who have spent years of their efforts slowly and laboriously in hand-engineering vision, audio or text features.~While much of this feature-engineering work is extremely clever, one may wonder if we can do better.

Ideally we would like to have algorithms that can automatically learn even better feature representations than the hand-engineered ones.~That is the reason why this paper describes the \textbf{sparse autoencoder} learning algorithm, which is one approach to learn features from unlabeled data autonomously.~In some domains, such as computer vision, this approach is not competitive by itself with the best hand-engineering features, but the features it can learn do turn out to be practical for a range of situations.